---
layout:     post                    # 使用的布局
title:      softmax回归——多分类问题           # 标题 
subtitle:   softmax解析 
date:       2019-06-09              # 时间
author:     Jinliang                      # 作者
header-img: img/post-bg-steam.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
mathjax: true                       #是否显示公式
tags:                               #标签
    - Deep Learning
---

在阅读这篇文章之前，强烈建议阅读先导篇：[logistic回归](https://jinliangxx.github.io/2019/05/16/logisitc%E5%9B%9E%E5%BD%92/)

## 1. softmax回归介绍

在[logistic回归](https://jinliangxx.github.io/2019/05/16/logisitc%E5%9B%9E%E5%BD%92/)一章中，我们解决的是二分类任务，如果不仅有两个分类，多分类任务怎么办？

在n分类任务中，我们最后的网络应该输出四个值，分别代表不同种类的值，我们现在要做的是将神经网络的输出值转化为对应每种类别的概率，就像LR一样。

此时我们需要一个softmax激活函数，它到底是怎样处理的呢？请看下面公式：


$$
t=e^{z^{[l]}}\\
a_i^{[l]}=\frac{t_i}{\sum_{j=1}^nt_i}\tag{1-1}
$$


在公式1-1中，$t$是我们引入的临时变量，相当于每个输出值都做操作：np.exp(z)；然后再归一化，求每个$t$所占的比例，即为每个类别的概率。


$$
\begin{bmatrix}
5\\
2\\
-1\\
3
\end{bmatrix}
\to
\begin{bmatrix}
e^5\\
e^2\\
e^{-1}\\
e^3
\end{bmatrix}\to
\begin{bmatrix}
0.842\\
0.042\\
0.002\\
0.114
\end{bmatrix}\tag{1-2}
$$
总结一下，其实softmax在使用时相当于一个激活函数，符合$a^{[l]}=g^{[l]}(z^{[l]})$的格式。

但是与[常见的激活函数](https://jinliangxx.github.io/2019/04/25/%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/)不同，一般的激活函数是以值为输入，softmax以向量为输入(需要求解概率)



## 2. softmax深入理解

首先看一下hardmax，别怀疑，真的有hardmax。

hardmax的做法是将最后的结果中最大的元素的位置放上1，其他位置放上0，具体例子如下：


$$
\begin{bmatrix}
5\\
2\\
-1\\
3
\end{bmatrix}
\to
\begin{bmatrix}
1\\
0\\
0\\
0
\end{bmatrix}\tag{2-1}
$$


过程2-1就是hardmax的过程，很简单粗暴，找出最大的值标记为1，其他为零。

softmax与hardmax的区别可以通过过程1-2和2-1观察出，softmax求解的是概率(其实有点像强化学习中$\pi(s)$与$\pi(s,a)$的区别)。

---

在这里将softmax与logistic做一下对比，也许他们的界限很明显，一个是多分类，一个是二分类。

但是实际上两者求解的都是概率，在softmax多分类的"多"等于"二"时，其实softmax就转化成了sigmoid。

我们来尝试证明一下：


$$
logistic:\qquad \qquad \qquad \qquad\\
\hat y=a=sigmoid(z)=\frac{1}{1+e^{-z}}\\
softmax:\qquad \qquad \qquad \qquad\\
\hat y_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}}\\
证明：\qquad \qquad \qquad \qquad\\
\hat y_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}}=\frac{1}{1+e^{z_2-z_1}}\\
令z_2=w_2x;\quad z_1=w_1x;\quad z=wx\\
则\hat y_1=\frac{1}{1+e^{(w_2-w_1)x}}\\
令w_2-w_1=-w，得证！\tag{2-2}
$$


---

softmax的损失函数：


$$
L(\hat y,y)=-\sum_{j=1}^ny_jlog\hat y_j \tag{2-3}
$$


我们来分析一下公式2-3，首先分析其中的$y$，在label中，仅有一个类别为1，其他为0，因此，虽然是求和操作，但是最终仅有真实类别的项能被保留；我们的任务是最小化损失函数，因为公式2-3前面有负号，因此在$y=1$时，对应的$\hat y$越大，我们的损失函数越小。相当于想要我们对应真实类别的概率尽可能大，就是[极大似然估计](https://jinliangxx.github.io/2019/06/03/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/#2-%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)的一种形式。

而且，对softmax的损失函数中的$n$换成2，发现又与logistic的损失函数对上了。

对应的成本函数就是一般的套路了：


$$
J(\textbf w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat y,y)\tag{2-4}
$$


---

成本函数求解出来，接下来就是BP过程了。

其中的难点就是对softmax求导，我们再来尝试求解一下：

当$i\neq j$时，令$S_i=\frac{e^{z_i}}{\sum}$:


$$
\begin{equation*}
\begin{split}
\frac{\partial \frac{e^{z_i}}{\sum_{k=1}^{N}e^{z_k}}}{\partial z_j}=
\frac{0-e^{z_j}e^{z_i}}{\sum^2 } \\
=-\frac{e^{z_j}}{\sum }\frac{e^{z_i}}{\sum} \\
=-S_jS_i
\end{split}
\end{equation*}\tag{2-5}
$$


当$i=j$时，


$$
\begin{equation*}
\begin{split}
\frac{\partial \frac{e^{z_i}}{\sum_{k=1}^{N}e^{z_k}}}{\partial z_j}=
\frac{e^{z_i}\sum-e^{z_j}e^{z_i}}{\sum^2 } \\
=\frac{e^{z_i}}{\sum }\frac{\sum-e^{z_j}}{\sum} \\
=S_i(1-S_j)
\end{split}
\end{equation*}\tag{2-6}
$$


将上述两种情况总结：


$$
\begin{equation*}
D_jS_i=\left\{\begin{matrix}
S_i(1-S_j) & i=j\\
-S_jS_i & i\neq j
\end{matrix}\right.
\end{equation*}\tag{2-7}
$$


以上就是softmax函数的求导过程，在知道softmax的导数后，就可以根据公式2-3的损失函数计算$\frac{dJ}{dz}$,具体细节同样是分类讨论，不涉及高难度的数据过程，不再赘述。

通过损失函数求得导数为：


$$
dz^{[l]}=\hat y-y\tag{2-8}
$$


根据公式2-8，即可进行BP过程的计算，更新参数。



## 3. 总结

softmax回归的函数较为简单，但是其原理却很复杂，特别是softmax与logistic的关系；以及使用softmax的反向求导过程。

如果有兴趣的话，可以尝试证明一下softmax与logistic的关系以及公式2-8，当然，如果有更好的证明思路也可以分享学习。