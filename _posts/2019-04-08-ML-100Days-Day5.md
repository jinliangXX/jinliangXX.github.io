---
layout:     post                    # 使用的布局
title:      第五天-机器学习100天           # 标题 
subtitle:   K近邻法(k-NN)
date:       2019-04-08              # 时间
author:     Jinliang                      # 作者
header-img: img/post-bg-coffee.jpeg    #这篇文章标题背景图片
catalog: true                       # 是否归档
mathjax: true                       #是否显示公式
tags:                               #标签
    - ML100Days

---

![知识谱图](https://ws4.sinaimg.cn/large/006tNc79ly1g1vmambhbwj30m81hwkfu.jpg)

# 数据集 | 社交网络

![数据集截图](https://ws3.sinaimg.cn/large/006tNc79ly1g1ugoxcbqwj30hz0j0glr.jpg)

# 导入相关库

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

# 导入数据集

```python
dataset = pd.read_csv('datasets/Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values
```

# 将数据划分成训练集和测试集

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
```

# 特征缩放

```python
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```

# 使用K-NN对训练集数据进行训练

```python
from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(X_train, y_train)
```

---

```
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')
```

# 对测试集进行预测

```python
y_pred = classifier.predict(X_test)
```

---

```
y_pred:[0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0
 0 0 1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1
 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 1 1 1]
```

# 生成混淆矩阵

```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print('cm: '  + str(cm))
```

---

```
cm: [[64  4]
 [ 3 29]]
```



---

---

# 知识点

KNN，即K近邻，是一种常用的**监督学习**方法。

**工作机制：**给定测试样本，根据*某种距离*度量出训练集中与测试样本最近的的k个训练样本，然后根据这k个邻居的信息来进行预测。

### **根据k个邻居的信息进行预测的机制：**

1. *投票法：*即k个样本中出现最多的类别标记作为预测结果，常用于**分类任务**中。
2. *平均法：*即去k个样本标记的平均值作为预测结果，常用于**回归任务**中。
3. 在投票或平均时，可进行**加权**，距离越近权重越大。 

### 影响分类结果的因素：

1. **超参数K**的选择

2. **距离计算方式**的选择

   

### 性能：

KNN算法虽然简单，但是性能很好，它的**泛化错误率不超过贝叶斯最优分类器错误率的两倍**，证明如下：

KNN出错的概率表示为：
$$
P(err)=1-\sum_{x\in{y}}P(c|x)P(c|z)
$$
假设样本独立同分布，且对任意测试样本，总能在任意近范围找到训练样本$z$，则贝**叶斯最优分类器**的结果为：
$$
\begin{align}
P(err)&=1-\sum_{c\in{y}}P(c|x)P(c|z)\\
&\approx1-\sum_{c\in{y}}P^2(c|x)\\
&\leq1-P^2(c^*|x)\\
&=(1+P(c^*|x))(1-P(c^*|x))\\
&\leq2*(1-P(c^*|x))
\end{align}
$$
因此，KNN的**泛化错误率不超过贝叶斯最优分类器错误率的两倍**。





> 部分转载：<https://github.com/MLEveryday/100-Days-Of-ML-Code>

